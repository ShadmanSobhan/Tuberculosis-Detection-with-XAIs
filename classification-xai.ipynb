{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":90999,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":76283,"modelId":100962}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Installation","metadata":{"id":"2OoHpCm2PzMy"}},{"cell_type":"code","source":"import tensorflow as tf\n\nif tf.config.list_physical_devices('GPU'):\n    print(\"GPU is available\")\nelse:\n    print(\"GPU is not available\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iRjiwk2FSK7r","outputId":"3a0d9bc7-2022-49e0-b94c-c2dc6aae9e0a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tensorflow==2.15","metadata":{"id":"a5FdGJd3yYsO","outputId":"b0e2172b-4c0b-4188-c23b-577363fabea2","colab":{"base_uri":"https://localhost:8080/","height":1000},"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install keras==2.15.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","metadata":{"id":"-h4-Xpaeya8u","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c375a3e4-581f-4d6f-ff38-0756047f5265","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import keras\nprint(keras.__version__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tensorflow-addons","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DZ9BwDaSPtV7","outputId":"3fe4dff7-1896-4b85-bf6c-7d73b72a0a68","scrolled":true,"execution":{"iopub.status.busy":"2024-08-20T05:23:06.768232Z","iopub.execute_input":"2024-08-20T05:23:06.768605Z","iopub.status.idle":"2024-08-20T05:23:19.009419Z","shell.execute_reply.started":"2024-08-20T05:23:06.768574Z","shell.execute_reply":"2024-08-20T05:23:19.008157Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.10/site-packages (0.23.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons) (21.3)\nRequirement already satisfied: typeguard<3.0.0,>=2.7 in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons) (2.13.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow-addons) (3.1.1)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# import system lib\nimport os\nimport time\nimport shutil\nimport pathlib\nimport itertools\n\n# import data handling tools\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# import Deep learning Libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\nfrom tensorflow.keras import regularizers\nimport tensorflow_addons as tfa\nfrom keras import backend as K\n# Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint ('modules loaded')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZUPoadj_PtbC","outputId":"4a106ca4-4ed7-4312-ec89-14b924d5034d","scrolled":true,"execution":{"iopub.status.busy":"2024-08-20T05:23:19.011829Z","iopub.execute_input":"2024-08-20T05:23:19.012683Z","iopub.status.idle":"2024-08-20T05:23:23.396165Z","shell.execute_reply.started":"2024-08-20T05:23:19.012645Z","shell.execute_reply":"2024-08-20T05:23:23.395191Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2024-08-20 05:23:20.383216: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-20 05:23:20.383266: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-20 05:23:20.384671: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"modules loaded\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Input","metadata":{"id":"E_1mQi7mx9Wz"}},{"cell_type":"code","source":"!kaggle datasets download -d shadmansobhan/tb-detection-4-august","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TegdhlvJ0HTK","outputId":"8c0de6d9-a70d-43ac-be67-2a6aaac13fc1","execution":{"iopub.status.busy":"2024-08-20T05:23:26.995145Z","iopub.execute_input":"2024-08-20T05:23:26.996299Z","iopub.status.idle":"2024-08-20T05:23:34.638970Z","shell.execute_reply.started":"2024-08-20T05:23:26.996263Z","shell.execute_reply":"2024-08-20T05:23:34.637834Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Warning: Looks like you're using an outdated API Version, please consider updating (server 1.6.17 / client 1.6.14)\nDataset URL: https://www.kaggle.com/datasets/shadmansobhan/tb-detection-4-august\nLicense(s): MIT\nDownloading tb-detection-4-august.zip to /kaggle/working\n 96%|██████████████████████████████████████▍ | 489M/508M [00:05<00:00, 61.2MB/s]\n100%|████████████████████████████████████████| 508M/508M [00:05<00:00, 95.4MB/s]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import zipfile\nimport os\n\n# Path to the zip file\nzip_file_path = '/kaggle/working/tb-detection-4-august.zip'\n\n# Path to the directory where the zip file will be extracted\nextract_to_path = '/kaggle/working/Image Files'\n\n# Extract the zip file\nwith zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n    zip_ref.extractall(extract_to_path)\n\nprint(f\"Extracted {zip_file_path} to {extract_to_path}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SNmihZxzNW4g","outputId":"43073c61-4da0-42d2-c696-62897489e17d","execution":{"iopub.status.busy":"2024-08-20T05:23:34.641171Z","iopub.execute_input":"2024-08-20T05:23:34.641677Z","iopub.status.idle":"2024-08-20T05:23:41.886406Z","shell.execute_reply.started":"2024-08-20T05:23:34.641639Z","shell.execute_reply":"2024-08-20T05:23:41.885456Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Extracted /kaggle/working/tb-detection-4-august.zip to /kaggle/working/Image Files\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Preprocessing","metadata":{"id":"DStiUGkyOitx"}},{"cell_type":"markdown","source":"### histogram & Clahe","metadata":{"id":"d6l2zevHA4Gz"}},{"cell_type":"code","source":"import cv2\nimport os\n\n# Define the folder path containing the images\nfolder_path = '/kaggle/working/Image Files/Merged_Custom'\n\n# Initialize a counter for the total number of preprocessed images\npreprocessed_count = 0\n\n# Recursively traverse the directory tree\nfor root, _, files in os.walk(folder_path):\n    for file in files:\n        image_path = os.path.join(root, file)\n\n        # Only process files with valid image extensions\n        if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n            # Check if the image was successfully loaded\n            if image is None:\n                print(f\"Image at path '{image_path}' not found.\")\n                continue\n\n            # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n            clahe_image = clahe.apply(image)\n\n            # Apply histogram equalization on the original image (not on CLAHE result)\n            hist_eq_image = cv2.equalizeHist(image)\n\n            # Apply CLAHE and then histogram equalization\n            hist_eq_then_clahe_image = clahe.apply(hist_eq_image)\n\n            # Save the modified image, replacing the original\n            cv2.imwrite(image_path, hist_eq_then_clahe_image)\n\n            # Increment the counter\n            preprocessed_count += 1\n\n            #print(f\"Processed and replaced image at path '{image_path}'\")\n\n# Print the total number of preprocessed images\nprint(f\"Total number of images preprocessed: {preprocessed_count}\")\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rrC1ZB-hwfwZ","outputId":"72e2d6a9-73ad-40df-cd9b-e971d3db005e","scrolled":true,"execution":{"iopub.status.busy":"2024-08-20T05:24:23.501464Z","iopub.execute_input":"2024-08-20T05:24:23.501845Z","iopub.status.idle":"2024-08-20T05:24:37.141774Z","shell.execute_reply.started":"2024-08-20T05:24:23.501817Z","shell.execute_reply":"2024-08-20T05:24:37.140690Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Total number of images preprocessed: 5000\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# **CSV FIle Generation based on Labelled Image**","metadata":{"id":"Ca8qlGFNS-9b"}},{"cell_type":"code","source":"data_dir = \"/kaggle/working/Image Files/Merged_Custom\"\nfilepaths = []\nlabels = []\n\nfolds = os.listdir(data_dir)\nfor fold in folds:\n    foldpath = os.path.join(data_dir, fold)\n    filelist = os.listdir(foldpath)\n    for file in filelist:\n        fpath = os.path.join(foldpath, file)\n        filepaths.append(fpath)\n        labels.append(fold)\n\n# Concatenate data paths with labels into one dataframe\nFseries = pd.Series(filepaths, name= 'filepaths')\nLseries = pd.Series(labels, name='labels')\ndf = pd.concat([Fseries, Lseries], axis= 1)","metadata":{"id":"rEPYj79QPtuK","execution":{"iopub.status.busy":"2024-08-20T05:24:37.143453Z","iopub.execute_input":"2024-08-20T05:24:37.143817Z","iopub.status.idle":"2024-08-20T05:24:37.174236Z","shell.execute_reply.started":"2024-08-20T05:24:37.143786Z","shell.execute_reply":"2024-08-20T05:24:37.173284Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"LRcZ6-f3Ptwz","outputId":"e333fcb3-481e-4a80-c9a6-063c278e6f23","execution":{"iopub.status.busy":"2024-08-20T05:24:37.936118Z","iopub.execute_input":"2024-08-20T05:24:37.936520Z","iopub.status.idle":"2024-08-20T05:24:37.960806Z","shell.execute_reply.started":"2024-08-20T05:24:37.936489Z","shell.execute_reply":"2024-08-20T05:24:37.959934Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                              filepaths        labels\n0     /kaggle/working/Image Files/Merged_Custom/Norm...        Normal\n1     /kaggle/working/Image Files/Merged_Custom/Norm...        Normal\n2     /kaggle/working/Image Files/Merged_Custom/Norm...        Normal\n3     /kaggle/working/Image Files/Merged_Custom/Norm...        Normal\n4     /kaggle/working/Image Files/Merged_Custom/Norm...        Normal\n...                                                 ...           ...\n4995  /kaggle/working/Image Files/Merged_Custom/Tube...  Tuberculosis\n4996  /kaggle/working/Image Files/Merged_Custom/Tube...  Tuberculosis\n4997  /kaggle/working/Image Files/Merged_Custom/Tube...  Tuberculosis\n4998  /kaggle/working/Image Files/Merged_Custom/Tube...  Tuberculosis\n4999  /kaggle/working/Image Files/Merged_Custom/Tube...  Tuberculosis\n\n[5000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filepaths</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/working/Image Files/Merged_Custom/Norm...</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/working/Image Files/Merged_Custom/Norm...</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/working/Image Files/Merged_Custom/Norm...</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/working/Image Files/Merged_Custom/Norm...</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/working/Image Files/Merged_Custom/Norm...</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4995</th>\n      <td>/kaggle/working/Image Files/Merged_Custom/Tube...</td>\n      <td>Tuberculosis</td>\n    </tr>\n    <tr>\n      <th>4996</th>\n      <td>/kaggle/working/Image Files/Merged_Custom/Tube...</td>\n      <td>Tuberculosis</td>\n    </tr>\n    <tr>\n      <th>4997</th>\n      <td>/kaggle/working/Image Files/Merged_Custom/Tube...</td>\n      <td>Tuberculosis</td>\n    </tr>\n    <tr>\n      <th>4998</th>\n      <td>/kaggle/working/Image Files/Merged_Custom/Tube...</td>\n      <td>Tuberculosis</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>/kaggle/working/Image Files/Merged_Custom/Tube...</td>\n      <td>Tuberculosis</td>\n    </tr>\n  </tbody>\n</table>\n<p>5000 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"print(\"Unique labels in the DataFrame:\")\nprint(df['labels'].unique())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LI-ZIGfATIVs","outputId":"d8ab5fd5-2e5a-4aa3-b2b6-f2e3698f97f6","execution":{"iopub.status.busy":"2024-08-20T05:24:42.689656Z","iopub.execute_input":"2024-08-20T05:24:42.690285Z","iopub.status.idle":"2024-08-20T05:24:42.698471Z","shell.execute_reply.started":"2024-08-20T05:24:42.690244Z","shell.execute_reply":"2024-08-20T05:24:42.697568Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Unique labels in the DataFrame:\n['Normal' 'Tuberculosis']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Splitting data for training","metadata":{"id":"x4IlDQLiTLM6"}},{"cell_type":"code","source":"# train dataframe\ntrain_df, test_df = train_test_split(df,  train_size= 0.8, shuffle= True, random_state= 1)\n\n# valid and test dataframe\n# valid_df, test_df = train_test_split(dummy_df,  train_size= 0.5, shuffle= True, random_state= 123)\n\nclass_counts_train = train_df['labels'].value_counts()\n\n# Print the counts for each class in the training set\nprint(\"Class counts in training set:\")\nprint(class_counts_train)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zyaXCbxaTIYi","outputId":"5e3fc2df-b25c-43de-d44b-cb2e6298cedb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\n\ndef tf_equalize_histogram(image):\n    values_range = tf.constant([0., 255.], dtype=tf.float32)\n\n    # Convert image to float32 using tf.cast\n    image_float = tf.cast(image, dtype=tf.float32)\n\n    histogram = tf.histogram_fixed_width(image_float, values_range, 256)\n    cdf = tf.cumsum(histogram)\n    cdf_min = cdf[tf.reduce_min(tf.where(tf.greater(cdf, 0)))]\n\n    img_shape = tf.shape(image)\n    pix_cnt = img_shape[-3] * img_shape[-2]\n    px_map = tf.round(tf.cast(cdf - cdf_min, dtype=tf.float32) * 255. / tf.cast(pix_cnt - 1, dtype=tf.float32))\n\n    px_map = tf.cast(px_map, dtype=tf.uint8)\n\n    # Use tf.gather to map values from px_map to the image\n    eq_hist_img = tf.gather(px_map, tf.cast(image_float, dtype=tf.int32))\n\n    return eq_hist_img\n\ndef preprocess_image(img):\n    # img = img / 255.0\n    img = tfa.image.equalize(img)\n    img = tf.image.adjust_contrast(img, contrast_factor=1.2)\n\n    # Normalize the image to the range [0, 1]\n    # img = img / 255.0\n\n    # Clip values to ensure they are in the valid range [0, 1]\n    # img = tf.clip_by_value(img, 0.0, 1.0)\n\n    # Convert the image to float32 if needed\n\n    img = tf.cast(img, dtype=tf.float32)\n\n    return img\n\n# cropped image size\nbatch_size = 16\nimg_size = (224, 224)\nchannels = 3\nimg_shape = (img_size[0], img_size[1], channels)\n\n# Recommended : use custom function for test data batch size, else we can use normal batch size.\nts_length = len(test_df)\ntest_batch_size = 8\n#max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\ntest_steps = ts_length // test_batch_size\n\n# This function which will be used in image data generator for data augmentation, it just take the image and return it again.\ndef scalar(img):\n    return img\n\ntr_gen = ImageDataGenerator(\n        # featurewise_center=True,\n        # featurewise_std_normalization= True,\n        horizontal_flip=False,  # Set to True for random horizontal flips\n        vertical_flip=False,  # Set to False to disable vertical flips\n        rotation_range=12,\n        zoom_range=0.2,\n        preprocessing_function = scalar\n        )\n\nts_gen = ImageDataGenerator(preprocessing_function= scalar)\n\ntrain_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n\n# valid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n#                                     color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n\n# Note: we will use custom test_batch_size, and make shuffle= false\ntest_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n                                    color_mode= 'rgb', shuffle= False, batch_size= test_batch_size)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Y3MtZTBTIbV","outputId":"cbdd4f25-84b7-41de-ac1d-a67ddc14ca40","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Sample from training data**","metadata":{"id":"4dOYw3RLTb1K"}},{"cell_type":"code","source":"g_dict = train_gen.class_indices     # defines dictionary {'class': index}\nclasses = list(g_dict.keys())       # defines list of dictionary's keys (classes), classes names : string\nimages, labels = next(train_gen)      # get a batch size samples from the generator\n\n\nprint(\"Class indices dictionary:\", g_dict)\nprint(\"List of class names:\", classes)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AYidbn9WTIeC","outputId":"962009bb-57d6-4cbd-ecd5-3cab367ffed9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize= (20, 20))\n\nfor i in range(16):\n    plt.subplot(4, 4, i + 1)\n    image = images[i] / 255       # scales data to range (0 - 255)\n    plt.imshow(image)\n    index = np.argmax(labels[i])  # get image index\n    class_name = classes[index]   # get class of image\n    plt.title(class_name, color= 'blue', fontsize= 12)\n    plt.axis('off')\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3We2K7IATIgy","outputId":"d537c5bd-afe1-423a-8163-c4c2e14f7c4d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\n\n# Example train_df for demonstration (replace with your actual data)\n# train_df = pd.read_csv('your_data.csv')\n\n# Get the value counts for the labels\nlabel_counts = train_df['labels'].value_counts()\n\n# Check the unique values in the labels\nif len(label_counts) < 2:\n    raise ValueError(\"There are not enough classes in the 'labels' column. Make sure there are at least two classes.\")\n\n# Extract the counts for each class\ntrain_norm = label_counts.iloc[0]\ntrain_tb = label_counts.iloc[1]\n\nprint('Percent of Tuberculosis/Normal : {} %'.format(100 * train_tb/train_norm))\n\n# Create positive frequency and negative frequency\nfreq_pos = round((train_tb / train_norm), 2)\nfreq_neg = 1 - freq_pos\n\npos_weights = freq_neg\nneg_weights = freq_pos\npos_contribution = freq_pos * pos_weights\nneg_contribution = freq_neg * neg_weights\n\nprint(pos_contribution, neg_contribution)\n\n# Create weighted loss function\ndef get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7):\n    def weighted_loss(y_true, y_pred):\n        # Initialize loss to zero\n        loss = 0.0\n        # For each class, add average weighted loss for that class\n        loss += K.mean(-(pos_weights * y_true * K.log(y_pred + epsilon) +\n                         neg_weights * (1 - y_true) * K.log(1 - y_pred + epsilon)))\n        return loss\n    return weighted_loss\n\n# Example usage of the weighted loss function\n# model.compile(optimizer='adam', loss=get_weighted_loss(pos_weights, neg_weights))","metadata":{"id":"vwS7TyBMTIjj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# strategy = tf.distribute.TPUStrategy(resolver)\ntrain_norm = train_df['labels'].value_counts()[0]\ntrain_tb = train_df['labels'].value_counts()[1]\n\nprint('Percent of Tuberculosis/Normal : {} %'.format(100 * train_tb/train_norm))\n\n#create positive frequency and negative frequency\nfreq_pos = round((train_tb/train_norm),2)\nfreq_neg = 1 - freq_pos\n\npos_weights = freq_neg\nneg_weights = freq_pos\npos_contribution = freq_pos * pos_weights\nneg_contribution = freq_neg * neg_weights\n\nprint(pos_contribution, neg_contribution)\n\n# create weight loss\ndef get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7):\n\n    def weighted_loss(y_true, y_pred):\n        # initialize loss to zero\n        loss = 0.0\n\n        # for each class, add average weighted loss for that class\n        loss += K.mean(-(pos_weights *y_true * K.log(y_pred + epsilon)\n                             + neg_weights* (1 - y_true) * K.log( 1 - y_pred + epsilon)))\n        return loss\n\n    return weighted_loss","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mqgEJhwSTnZi","outputId":"c3ccfbf9-59c7-4398-e36d-8533b9d517aa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Creation","metadata":{"id":"rGLBn5XiTtnb"}},{"cell_type":"markdown","source":"### Eff Net","metadata":{"id":"tsEjFx5gkiHh"}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\n# Create Model Structure\nimg_size = (224, 224)\nchannels = 3\nimg_shape = (img_size[0], img_size[1], channels)\nclass_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer\n\n\n# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n# we will use efficientnetb3 from EfficientNet family.\nbase_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n# base_model.trainable = False\n\n\nmodel = Sequential([\n        base_model,\n        BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n        Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n                    bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n        Dropout(rate= 0.45, seed= 123),\n        Dense(class_count, activation= 'softmax')\n])\n\nclass CustomModelCheckpoint(Callback):\n    def __init__(self, filepath, monitor='val_accuracy', mode='max', verbose=1):\n        super(CustomModelCheckpoint, self).__init__()\n        self.filepath = filepath\n        self.monitor = monitor\n        self.verbose = verbose\n        self.mode = mode\n        self.best_val_acc = -np.Inf if mode == 'max' else np.Inf\n        self.best_train_acc = -np.Inf\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        current_val_acc = logs.get(self.monitor)\n        current_train_acc = logs.get('accuracy')\n\n        if current_val_acc is None or current_train_acc is None:\n            return\n\n        # Check if current validation accuracy is the best seen so far\n        if current_val_acc > self.best_val_acc:\n            self.best_val_acc = current_val_acc\n            self.best_train_acc = current_train_acc\n            self.model.save_weights(self.filepath)\n            if self.verbose > 0:\n                print(f'\\nEpoch {epoch + 1}: {self.monitor} improved to {current_val_acc}, saving model.')\n\n        # If the validation accuracy is equal, check for higher training accuracy\n        elif current_val_acc == self.best_val_acc and current_train_acc > self.best_train_acc:\n            self.best_train_acc = current_train_acc\n            self.model.save_weights(self.filepath)\n            if self.verbose > 0:\n                print(f'\\nEpoch {epoch + 1}: {self.monitor} same as best, but training accuracy improved to {current_train_acc}, saving model.')\n\n# Usage in model training\ncheckpoint = CustomModelCheckpoint(\n    filepath='best_weights',\n    monitor='val_accuracy',\n    mode='max',\n    verbose=1\n)\n\nearly = EarlyStopping(monitor=\"val_loss\",\n                      mode=\"min\",\n                      patience=8)\n\n\nmodel.compile(Adamax(learning_rate= 0.001), loss=get_weighted_loss(pos_weights, neg_weights) , metrics= ['accuracy'])\n\nmodel.summary()\n\nfrom tensorflow.keras.utils import plot_model\n\n# Assuming 'model' is your defined model\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n#'categorical_crossentropy'","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"aY1JeKvEklMn","outputId":"48fa3f7a-a768-4bcf-a5fd-21f04ae0e74f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_lrfn(lr_start=0.000001, lr_max=0.001,\n               lr_min=0, lr_rampup_epochs=8,\n               lr_sustain_epochs=0, lr_exp_decay=.8):\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) *\\\n                 lr_exp_decay**(epoch - lr_rampup_epochs\\\n                                - lr_sustain_epochs) + lr_min\n        return lr\n    return lrfn\n\nlrfn = build_lrfn()\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)","metadata":{"id":"J40mz2dDTnhK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{"id":"a3DtK4ADT5kd"}},{"cell_type":"code","source":"batch_size = 16   # set batch size for training\n# epochs = 100   # number of all epochs in training\n\n\n# history = model.fit( x=train_gen,\n#                     epochs= epochs,\n#                     verbose= 1,\n#                     validation_data= valid_gen,\n#                     validation_steps= None,\n#                     callbacks=[lr_schedule, early, checkpoint],\n#                     shuffle= False)\n\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.models import load_model\n\nhistory_data=[]\n\n# Define the number of folds for cross-validation\nnum_folds = 5\n\n# Initialize k-fold cross-validation\nskf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n\n# Initialize variables to store results\nall_accuracies = []\n\n# Perform k-fold cross-validation\nfold_count = 0\nfor train_index, valid_index in skf.split(train_df, train_df['labels']):\n    fold_count += 1\n    print(f\"\\nTraining Fold {fold_count}\")\n\n    # Split data into training and validation sets\n    train_fold, valid_fold = train_df.iloc[train_index], train_df.iloc[valid_index]\n\n    # Data generators for this fold\n    # train_gen = tr_gen.flow_from_dataframe(train_fold, x_col='filepaths', y_col='labels', target_size=img_size,\n    #                                        class_mode='categorical', color_mode='rgb', shuffle=True, batch_size=batch_size)\n\n    # valid_gen = ts_gen.flow_from_dataframe(valid_fold, x_col='filepaths', y_col='labels', target_size=img_size,\n    #                                        class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=batch_size)\n    train_gen = tr_gen.flow_from_dataframe( train_fold, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n\n    valid_gen = ts_gen.flow_from_dataframe( valid_fold, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)\n\n    # Load a new instance of the model for each fold\n\n\n    # Train the model\n\n    history = model.fit(x=train_gen, epochs=75, verbose=1, validation_data=valid_gen, validation_steps=None,\n                        callbacks=[lr_schedule, early, checkpoint], shuffle=False)\n    history_data.append(history)\n    # Evaluate on the validation set of this fold\n\n    valid_accuracy = model.evaluate(valid_gen)[1]\n    all_accuracies.append(valid_accuracy)\n\n# Calculate and print the average accuracy across all folds\naverage_accuracy = np.mean(all_accuracies)\nprint(f'\\nAverage Accuracy across all Folds: {average_accuracy}')\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e0vrdEqXTnjb","outputId":"bc258a04-479c-4a37-b19f-9f9dbb5be0f3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Weight Load","metadata":{"id":"t05upL7RO9Zy"}},{"cell_type":"code","source":"#model.save_weights('/kaggle/input/weight-of-sm/tensorflow2/default/1')","metadata":{"id":"alcOrHVYLWjY","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#model.load_weights(\"/kaggle/input/weight-of-sm/tensorflow2/default/1/sm.h5\")\nmodel.load_weights('best_weights');","metadata":{"id":"ZZUI7DFmOQCa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Performance","metadata":{"id":"MOJMLf2lUFHz"}},{"cell_type":"code","source":"tr_acc=[]\ntr_loss=[]\nval_acc=[]\nval_loss=[]\nfor history in history_data:\n  n=len(history.history['accuracy'])\n  for i in range(n):\n    tr_acc.append(history.history['accuracy'][i])\n    tr_loss.append(history.history['loss'][i])\n    val_acc.append(history.history['val_accuracy'][i])\n    val_loss.append(history.history['val_loss'][i])\n\nindex_loss = np.argmin(val_loss)\nval_lowest = val_loss[index_loss]\nindex_acc = np.argmax(val_acc)\nacc_highest = val_acc[index_acc]\nEpochs = [i+1 for i in range(len(tr_acc))]\nloss_label = f'best epoch= {str(index_loss + 1)}'\nacc_label = f'best epoch= {str(index_acc + 1)}'\n\n# Plot training history\nplt.figure(figsize= (20, 8))\nplt.style.use('fivethirtyeight')\n\nplt.subplot(1, 2, 1)\nplt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\nplt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\nplt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\nplt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\nplt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout\nplt.show()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":679},"id":"Bck-TidHTnpK","outputId":"d3755023-4b83-48de-cb58-f5cbf61ba1a0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Evaluation**","metadata":{"id":"2A1P-YS7UMba"}},{"cell_type":"code","source":"ts_length = len(test_df)\ntest_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\ntest_steps = ts_length // test_batch_size\n\ntrain_score = model.evaluate(train_gen, steps= test_steps, verbose= 1)\nvalid_score = model.evaluate(valid_gen, steps= test_steps, verbose= 1)\ntest_score = model.evaluate(test_gen, steps= test_steps, verbose= 1)\n\nprint(\"Train Loss: \", train_score[0])\nprint(\"Train Accuracy: \", train_score[1])\nprint('-' * 20)\nprint(\"Validation Loss: \", valid_score[0])\nprint(\"Validation Accuracy: \", valid_score[1])\nprint('-' * 20)\nprint(\"Test Loss: \", test_score[0])\nprint(\"Test Accuracy: \", test_score[1])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lb5TeIwoUMmS","outputId":"8070da64-33fc-4385-f371-ae1b6dbe6e31","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Predictions","metadata":{"id":"fG43I0lYURra"}},{"cell_type":"code","source":"preds = model.predict_generator(test_gen)\ny_pred = np.argmax(preds, axis=1)","metadata":{"id":"YQSvHV7TdVeK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Report**","metadata":{"id":"W7P9m8-IUUEt"}},{"cell_type":"code","source":"g_dict = test_gen.class_indices\nclasses = list(g_dict.keys())\n\n# Confusion matrix\ncm = confusion_matrix(test_gen.classes, y_pred)\n\nplt.figure(figsize= (10, 10))\nplt.imshow(cm, interpolation= 'nearest', cmap= plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\n\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation= 45)\nplt.yticks(tick_marks, classes)\n\n\nthresh = cm.max() / 2.\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    plt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')\n\nplt.tight_layout()\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\n\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":957},"id":"FAWtOJsUUMsC","outputId":"273bf4b6-8e3e-4b25-ee08-42d21915ca4a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Classification report\nprint(classification_report(test_gen.classes, y_pred, target_names= classes))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Umrml50NUMuq","outputId":"0f6f87b1-893f-4a3e-fd3d-7f23fe286c41","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XAI","metadata":{}},{"cell_type":"code","source":"!pip install lime\n!pip install shap","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# # # # 64","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom lime import lime_image\nfrom skimage.segmentation import mark_boundaries\nfrom PIL import Image\n\n# Initialize LIME image explainer\nexplainer = lime_image.LimeImageExplainer()\n\n# Example preprocessing function (replace with your preprocessing logic)\ndef preprocess_image(img_path):\n    # Load and preprocess your image (example assumes resizing to 224x224)\n    img = Image.open(img_path)\n    img = img.convert('RGB')  # Ensure RGB mode\n    img = img.resize((224, 224))  # Resize to model input size\n    img = np.array(img)  # Convert PIL image to numpy array\n    return img\n\n# Example prediction function (replace with your model prediction function)\ndef predict_fn(images):\n    # Assuming 'model' is your trained Keras model\n    return model.predict(images)\n\n# Select an image to explain\nimg_path = '/kaggle/working/Image Files/Shenzhen_Custom_Masks/Normal/CHNCXR_0064_0.png'\nimg = preprocess_image(img_path)\n\n# Ensure the image is in the correct shape for LIME\nimg = img.astype(np.float32) / 255.0  # Normalize to [0, 1] if needed\n\n# Explain the prediction\nexplanation = explainer.explain_instance(\n    image=img,\n    classifier_fn=predict_fn,\n    top_labels=1,\n    hide_color=0,\n    num_samples=1000\n)\n\n# Get the explanation for the top class\ntemp, mask = explanation.get_image_and_mask(\n    label=explanation.top_labels[0],\n    positive_only=False,\n    num_features=5,\n    hide_rest=False\n)\n\n# Overlay the explanation mask on the original image\nimg_with_mask = mark_boundaries(np.array(img), mask)\n\n# Show the original image with overlaid ROI\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(np.array(img))\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(img_with_mask)\nplt.title('Image with ROI')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\nimport shap\nimport matplotlib.pyplot as plt\n\n# Define the image size\nimg_size = (224, 224)  # Adjust this to match your model's input size\n\n# Define the image path\nimg_path = '/kaggle/working/Image Files/Shenzhen_Custom_Masks/Normal/CHNCXR_0064_0.png'\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0010_0.png\"\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0028_0.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0327_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0333_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0360_1.png\"\n\n# Load and preprocess the image\nimg = image.load_img(img_path, target_size=img_size)\nimg_array = image.img_to_array(img)\nimg_array = np.expand_dims(img_array, axis=0)\nimg_array = tf.keras.applications.efficientnet.preprocess_input(img_array)\n\n# Make a prediction\npredictions = model.predict(img_array)\n\n# Get the class with the highest probability\npredicted_class = np.argmax(predictions, axis=1)\n\n# Assuming you have a list of class names\nclass_names = list(train_gen.class_indices.keys())  # Ensure train_gen is defined\npredicted_class_name = class_names[predicted_class[0]]\n\nprint(f'The predicted class is: {predicted_class_name}')\n\n# Define the function for SHAP\ndef f(x):\n    tmp = x.copy()\n    return model(tmp)\n\n# Create the masker using the shape of the loaded image\nmasker_blur = shap.maskers.Image(\"blur(224,224)\", img_array.shape[1:])\n\n# Create the explainer\nexplainer = shap.Explainer(f, masker_blur, output_names=class_names)\n\n# Compute SHAP values\nshap_values = explainer(img_array, max_evals=5000, batch_size=50)\n\n# Normalize SHAP values to fall within the range [0, 1]\n#shap_values.values = np.clip(shap_values.values, -1, 1)\n#shap_values.values = (shap_values.values - shap_values.values.min()) / (shap_values.values.max() - shap_values.values.min())\n\n# Normalize the image array to [0, 1] range for plotting\nimg_array_display = (img_array - img_array.min()) / (img_array.max() - img_array.min())\n\n# Plot the SHAP values\nshap.image_plot(shap_values, img_array_display, labels=class_names)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef make_gradcampp_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    grad_model = tf.keras.models.Model(\n        [model.inputs],\n        [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    with tf.GradientTape() as tape1:\n        with tf.GradientTape() as tape2:\n            with tf.GradientTape() as tape3:\n                last_conv_layer_output, preds = grad_model(img_array)\n                if pred_index is None:\n                    pred_index = tf.argmax(preds[0])\n                class_channel = preds[:, pred_index]\n\n            grads = tape3.gradient(class_channel, last_conv_layer_output)\n        grads2 = tape2.gradient(class_channel, last_conv_layer_output)\n    grads3 = tape1.gradient(class_channel, last_conv_layer_output)\n\n    alpha_num = grads2**2\n    alpha_denom = 2 * grads2**2 + tf.reduce_sum(last_conv_layer_output * grads3, axis=(1, 2))\n    alpha_denom = tf.where(alpha_denom != 0, alpha_denom, tf.ones(alpha_denom.shape))\n    alphas = alpha_num / alpha_denom\n    weights = tf.reduce_sum(alphas * tf.maximum(grads, 0), axis=(1, 2))\n\n    last_conv_layer_output = last_conv_layer_output[0]\n    cam = tf.reduce_sum(weights[:, tf.newaxis, tf.newaxis] * last_conv_layer_output, axis=-1)\n\n    heatmap = tf.maximum(cam, 0) / tf.math.reduce_max(cam)\n    heatmap = tf.image.resize(heatmap[..., tf.newaxis], (img_array.shape[1], img_array.shape[2]))\n    heatmap = tf.squeeze(heatmap)\n\n    return heatmap.numpy()\n\ndef display_gradcam(img_path, heatmap, alpha=0.4, threshold_percentile=90):\n    # Load and preprocess the original image\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Resize heatmap to match the original image size\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n\n    # Apply dynamic threshold to the heatmap based on percentile\n    threshold_value = np.percentile(heatmap, threshold_percentile)\n    heatmap = np.maximum(heatmap - threshold_value, 0)\n    heatmap = heatmap / np.max(heatmap)  # Normalize the heatmap\n\n    # Apply colormap to the heatmap\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)  # Convert heatmap to RGB for blending\n\n    # Blend heatmap with the original image\n    blended_img = (heatmap * alpha) + (img * (1 - alpha))\n    blended_img = np.clip(blended_img, 0, 255).astype(np.uint8)\n\n    # Plotting\n    plt.figure(figsize=(15, 5))\n\n    # Plot original image\n    plt.subplot(1, 3, 1)\n    plt.imshow(img)\n    plt.title('Original Image')\n    plt.axis('off')\n\n    # Plot heatmap\n    plt.subplot(1, 3, 2)\n    plt.imshow(heatmap)\n    plt.title('Heatmap')\n    plt.axis('off')\n\n    # Plot original image with heatmap overlay\n    plt.subplot(1, 3, 3)\n    plt.imshow(blended_img)\n    plt.title('Image with Heatmap Overlay')\n    plt.axis('off')\n\n    plt.show()\n\ndef get_last_conv_layer_name(model):\n    for layer in reversed(model.layers):\n        if isinstance(layer, tf.keras.layers.Conv2D):\n            return layer.name\n    return None\n\n# Example usage\nimg_path = '/kaggle/working/Image Files/Shenzhen_Custom_Masks/Normal/CHNCXR_0064_0.png'\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0010_0.png\"\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0028_0.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0327_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0333_1.png\"\n#img_path = \"/content/Shenzhen/Tuberculosis/CHNCXR_0360_1.png\"\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\n# Assuming 'base_model' is your model instance\nlast_conv_layer_name = get_last_conv_layer_name(base_model)\nprint(\"Last convolutional layer name:\", last_conv_layer_name)\n\n# Create the heatmap\nheatmap = make_gradcampp_heatmap(x, base_model, last_conv_layer_name)\n\ndisplay_gradcam(img_path, heatmap)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# # # # 343","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom lime import lime_image\nfrom skimage.segmentation import mark_boundaries\nfrom PIL import Image\n\n# Initialize LIME image explainer\nexplainer = lime_image.LimeImageExplainer()\n\n# Example preprocessing function (replace with your preprocessing logic)\ndef preprocess_image(img_path):\n    # Load and preprocess your image (example assumes resizing to 224x224)\n    img = Image.open(img_path)\n    img = img.convert('RGB')  # Ensure RGB mode\n    img = img.resize((224, 224))  # Resize to model input size\n    img = np.array(img)  # Convert PIL image to numpy array\n    return img\n\n# Example prediction function (replace with your model prediction function)\ndef predict_fn(images):\n    # Assuming 'model' is your trained Keras model\n    return model.predict(images)\n\n# Select an image to explain\nimg_path = '/kaggle/working/Image Files/Shenzhen_Custom_Masks/Tuberculosis/CHNCXR_0343_1.png'\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0010_0.png\"\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0028_0.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0327_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0333_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0360_1.png\"\nimg = preprocess_image(img_path)\n\n# Ensure the image is in the correct shape for LIME\nimg = img.astype(np.float32) / 255.0  # Normalize to [0, 1] if needed\n\n# Explain the prediction\nexplanation = explainer.explain_instance(\n    image=img,\n    classifier_fn=predict_fn,\n    top_labels=1,\n    hide_color=0,\n    num_samples=1000\n)\n\n# Get the explanation for the top class\ntemp, mask = explanation.get_image_and_mask(\n    label=explanation.top_labels[0],\n    positive_only=False,\n    num_features=5,\n    hide_rest=False\n)\n\n# Overlay the explanation mask on the original image\nimg_with_mask = mark_boundaries(np.array(img), mask)\n\n# Show the original image with overlaid ROI\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(np.array(img))\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(img_with_mask)\nplt.title('Image with ROI')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\nimport shap\nimport matplotlib.pyplot as plt\n\n# Define the image size\nimg_size = (224, 224)  # Adjust this to match your model's input size\n\n# Define the image path\nimg_path = '/kaggle/working/Image Files/Shenzhen_Custom_Masks/Tuberculosis/CHNCXR_0343_1.png'\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0010_0.png\"\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0028_0.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0327_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0333_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0360_1.png\"\n\n# Load and preprocess the image\nimg = image.load_img(img_path, target_size=img_size)\nimg_array = image.img_to_array(img)\nimg_array = np.expand_dims(img_array, axis=0)\nimg_array = tf.keras.applications.efficientnet.preprocess_input(img_array)\n\n# Make a prediction\npredictions = model.predict(img_array)\n\n# Get the class with the highest probability\npredicted_class = np.argmax(predictions, axis=1)\n\n# Assuming you have a list of class names\nclass_names = list(train_gen.class_indices.keys())  # Ensure train_gen is defined\npredicted_class_name = class_names[predicted_class[0]]\n\nprint(f'The predicted class is: {predicted_class_name}')\n\n# Define the function for SHAP\ndef f(x):\n    tmp = x.copy()\n    return model(tmp)\n\n# Create the masker using the shape of the loaded image\nmasker_blur = shap.maskers.Image(\"blur(224,224)\", img_array.shape[1:])\n\n# Create the explainer\nexplainer = shap.Explainer(f, masker_blur, output_names=class_names)\n\n# Compute SHAP values\nshap_values = explainer(img_array, max_evals=5000, batch_size=50)\n\n# Normalize SHAP values to fall within the range [0, 1]\n#shap_values.values = np.clip(shap_values.values, -1, 1)\n#shap_values.values = (shap_values.values - shap_values.values.min()) / (shap_values.values.max() - shap_values.values.min())\n\n# Normalize the image array to [0, 1] range for plotting\nimg_array_display = (img_array - img_array.min()) / (img_array.max() - img_array.min())\n\n# Plot the SHAP values\nshap.image_plot(shap_values, img_array_display, labels=class_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef make_gradcampp_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    grad_model = tf.keras.models.Model(\n        [model.inputs],\n        [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    with tf.GradientTape() as tape1:\n        with tf.GradientTape() as tape2:\n            with tf.GradientTape() as tape3:\n                last_conv_layer_output, preds = grad_model(img_array)\n                if pred_index is None:\n                    pred_index = tf.argmax(preds[0])\n                class_channel = preds[:, pred_index]\n\n            grads = tape3.gradient(class_channel, last_conv_layer_output)\n        grads2 = tape2.gradient(class_channel, last_conv_layer_output)\n    grads3 = tape1.gradient(class_channel, last_conv_layer_output)\n\n    alpha_num = grads2**2\n    alpha_denom = 2 * grads2**2 + tf.reduce_sum(last_conv_layer_output * grads3, axis=(1, 2))\n    alpha_denom = tf.where(alpha_denom != 0, alpha_denom, tf.ones(alpha_denom.shape))\n    alphas = alpha_num / alpha_denom\n    weights = tf.reduce_sum(alphas * tf.maximum(grads, 0), axis=(1, 2))\n\n    last_conv_layer_output = last_conv_layer_output[0]\n    cam = tf.reduce_sum(weights[:, tf.newaxis, tf.newaxis] * last_conv_layer_output, axis=-1)\n\n    heatmap = tf.maximum(cam, 0) / tf.math.reduce_max(cam)\n    heatmap = tf.image.resize(heatmap[..., tf.newaxis], (img_array.shape[1], img_array.shape[2]))\n    heatmap = tf.squeeze(heatmap)\n\n    return heatmap.numpy()\n\ndef display_gradcam(img_path, heatmap, alpha=0.4, threshold_percentile=90):\n    # Load and preprocess the original image\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Resize heatmap to match the original image size\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n\n    # Apply dynamic threshold to the heatmap based on percentile\n    threshold_value = np.percentile(heatmap, threshold_percentile)\n    heatmap = np.maximum(heatmap - threshold_value, 0)\n    heatmap = heatmap / np.max(heatmap)  # Normalize the heatmap\n\n    # Apply colormap to the heatmap\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)  # Convert heatmap to RGB for blending\n\n    # Blend heatmap with the original image\n    blended_img = (heatmap * alpha) + (img * (1 - alpha))\n    blended_img = np.clip(blended_img, 0, 255).astype(np.uint8)\n\n    # Plotting\n    plt.figure(figsize=(15, 5))\n\n    # Plot original image\n    plt.subplot(1, 3, 1)\n    plt.imshow(img)\n    plt.title('Original Image')\n    plt.axis('off')\n\n    # Plot heatmap\n    plt.subplot(1, 3, 2)\n    plt.imshow(heatmap)\n    plt.title('Heatmap')\n    plt.axis('off')\n\n    # Plot original image with heatmap overlay\n    plt.subplot(1, 3, 3)\n    plt.imshow(blended_img)\n    plt.title('Image with Heatmap Overlay')\n    plt.axis('off')\n\n    plt.show()\n\ndef get_last_conv_layer_name(model):\n    for layer in reversed(model.layers):\n        if isinstance(layer, tf.keras.layers.Conv2D):\n            return layer.name\n    return None\n\n# Example usage\nimg_path = '/kaggle/working/Image Files/Shenzhen_Custom_Masks/Tuberculosis/CHNCXR_0343_1.png'\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0010_0.png\"\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0028_0.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0327_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0333_1.png\"\n#img_path = \"/content/Shenzhen/Tuberculosis/CHNCXR_0360_1.png\"\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\n# Assuming 'base_model' is your model instance\nlast_conv_layer_name = get_last_conv_layer_name(base_model)\nprint(\"Last convolutional layer name:\", last_conv_layer_name)\n\n# Create the heatmap\nheatmap = make_gradcampp_heatmap(x, base_model, last_conv_layer_name)\n\ndisplay_gradcam(img_path, heatmap)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# # # # 330","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom lime import lime_image\nfrom skimage.segmentation import mark_boundaries\nfrom PIL import Image\n\n# Initialize LIME image explainer\nexplainer = lime_image.LimeImageExplainer()\n\n# Example preprocessing function (replace with your preprocessing logic)\ndef preprocess_image(img_path):\n    # Load and preprocess your image (example assumes resizing to 224x224)\n    img = Image.open(img_path)\n    img = img.convert('RGB')  # Ensure RGB mode\n    img = img.resize((224, 224))  # Resize to model input size\n    img = np.array(img)  # Convert PIL image to numpy array\n    return img\n\n# Example prediction function (replace with your model prediction function)\ndef predict_fn(images):\n    # Assuming 'model' is your trained Keras model\n    return model.predict(images)\n\n# Select an image to explain\nimg_path = '/kaggle/working/Image Files/Shenzhen_Custom_Masks/Tuberculosis/CHNCXR_0330_1.png'\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0010_0.png\"\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0028_0.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0327_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0333_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0360_1.png\"\nimg = preprocess_image(img_path)\n\n# Ensure the image is in the correct shape for LIME\nimg = img.astype(np.float32) / 255.0  # Normalize to [0, 1] if needed\n\n# Explain the prediction\nexplanation = explainer.explain_instance(\n    image=img,\n    classifier_fn=predict_fn,\n    top_labels=1,\n    hide_color=0,\n    num_samples=1000\n)\n\n# Get the explanation for the top class\ntemp, mask = explanation.get_image_and_mask(\n    label=explanation.top_labels[0],\n    positive_only=False,\n    num_features=5,\n    hide_rest=False\n)\n\n# Overlay the explanation mask on the original image\nimg_with_mask = mark_boundaries(np.array(img), mask)\n\n# Show the original image with overlaid ROI\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(np.array(img))\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(img_with_mask)\nplt.title('Image with ROI')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\nimport shap\nimport matplotlib.pyplot as plt\n\n# Define the image size\nimg_size = (224, 224)  # Adjust this to match your model's input size\n\n# Define the image path\nimg_path = '/kaggle/working/Image Files/Shenzhen_Custom_Masks/Tuberculosis/CHNCXR_0330_1.png'\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0010_0.png\"\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0028_0.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0327_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0333_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0360_1.png\"\n\n# Load and preprocess the image\nimg = image.load_img(img_path, target_size=img_size)\nimg_array = image.img_to_array(img)\nimg_array = np.expand_dims(img_array, axis=0)\nimg_array = tf.keras.applications.efficientnet.preprocess_input(img_array)\n\n# Make a prediction\npredictions = model.predict(img_array)\n\n# Get the class with the highest probability\npredicted_class = np.argmax(predictions, axis=1)\n\n# Assuming you have a list of class names\nclass_names = list(train_gen.class_indices.keys())  # Ensure train_gen is defined\npredicted_class_name = class_names[predicted_class[0]]\n\nprint(f'The predicted class is: {predicted_class_name}')\n\n# Define the function for SHAP\ndef f(x):\n    tmp = x.copy()\n    return model(tmp)\n\n# Create the masker using the shape of the loaded image\nmasker_blur = shap.maskers.Image(\"blur(224,224)\", img_array.shape[1:])\n\n# Create the explainer\nexplainer = shap.Explainer(f, masker_blur, output_names=class_names)\n\n# Compute SHAP values\nshap_values = explainer(img_array, max_evals=5000, batch_size=50)\n\n# Normalize SHAP values to fall within the range [0, 1]\n#shap_values.values = np.clip(shap_values.values, -1, 1)\n#shap_values.values = (shap_values.values - shap_values.values.min()) / (shap_values.values.max() - shap_values.values.min())\n\n# Normalize the image array to [0, 1] range for plotting\nimg_array_display = (img_array - img_array.min()) / (img_array.max() - img_array.min())\n\n# Plot the SHAP values\nshap.image_plot(shap_values, img_array_display, labels=class_names)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef make_gradcampp_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    grad_model = tf.keras.models.Model(\n        [model.inputs],\n        [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    with tf.GradientTape() as tape1:\n        with tf.GradientTape() as tape2:\n            with tf.GradientTape() as tape3:\n                last_conv_layer_output, preds = grad_model(img_array)\n                if pred_index is None:\n                    pred_index = tf.argmax(preds[0])\n                class_channel = preds[:, pred_index]\n\n            grads = tape3.gradient(class_channel, last_conv_layer_output)\n        grads2 = tape2.gradient(class_channel, last_conv_layer_output)\n    grads3 = tape1.gradient(class_channel, last_conv_layer_output)\n\n    alpha_num = grads2**2\n    alpha_denom = 2 * grads2**2 + tf.reduce_sum(last_conv_layer_output * grads3, axis=(1, 2))\n    alpha_denom = tf.where(alpha_denom != 0, alpha_denom, tf.ones(alpha_denom.shape))\n    alphas = alpha_num / alpha_denom\n    weights = tf.reduce_sum(alphas * tf.maximum(grads, 0), axis=(1, 2))\n\n    last_conv_layer_output = last_conv_layer_output[0]\n    cam = tf.reduce_sum(weights[:, tf.newaxis, tf.newaxis] * last_conv_layer_output, axis=-1)\n\n    heatmap = tf.maximum(cam, 0) / tf.math.reduce_max(cam)\n    heatmap = tf.image.resize(heatmap[..., tf.newaxis], (img_array.shape[1], img_array.shape[2]))\n    heatmap = tf.squeeze(heatmap)\n\n    return heatmap.numpy()\n\ndef display_gradcam(img_path, heatmap, alpha=0.4, threshold_percentile=90):\n    # Load and preprocess the original image\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Resize heatmap to match the original image size\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n\n    # Apply dynamic threshold to the heatmap based on percentile\n    threshold_value = np.percentile(heatmap, threshold_percentile)\n    heatmap = np.maximum(heatmap - threshold_value, 0)\n    heatmap = heatmap / np.max(heatmap)  # Normalize the heatmap\n\n    # Apply colormap to the heatmap\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)  # Convert heatmap to RGB for blending\n\n    # Blend heatmap with the original image\n    blended_img = (heatmap * alpha) + (img * (1 - alpha))\n    blended_img = np.clip(blended_img, 0, 255).astype(np.uint8)\n\n    # Plotting\n    plt.figure(figsize=(15, 5))\n\n    # Plot original image\n    plt.subplot(1, 3, 1)\n    plt.imshow(img)\n    plt.title('Original Image')\n    plt.axis('off')\n\n    # Plot heatmap\n    plt.subplot(1, 3, 2)\n    plt.imshow(heatmap)\n    plt.title('Heatmap')\n    plt.axis('off')\n\n    # Plot original image with heatmap overlay\n    plt.subplot(1, 3, 3)\n    plt.imshow(blended_img)\n    plt.title('Image with Heatmap Overlay')\n    plt.axis('off')\n\n    plt.show()\n\ndef get_last_conv_layer_name(model):\n    for layer in reversed(model.layers):\n        if isinstance(layer, tf.keras.layers.Conv2D):\n            return layer.name\n    return None\n\n# Example usage\nimg_path = '/kaggle/working/Image Files/Shenzhen_Custom_Masks/Tuberculosis/CHNCXR_0330_1.png'\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0010_0.png\"\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0028_0.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0327_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0333_1.png\"\n#img_path = \"/content/Shenzhen/Tuberculosis/CHNCXR_0360_1.png\"\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\n# Assuming 'base_model' is your model instance\nlast_conv_layer_name = get_last_conv_layer_name(base_model)\nprint(\"Last convolutional layer name:\", last_conv_layer_name)\n\n# Create the heatmap\nheatmap = make_gradcampp_heatmap(x, base_model, last_conv_layer_name)\n\ndisplay_gradcam(img_path, heatmap)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# # # # 78","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom lime import lime_image\nfrom skimage.segmentation import mark_boundaries\nfrom PIL import Image\n\n# Initialize LIME image explainer\nexplainer = lime_image.LimeImageExplainer()\n\n# Example preprocessing function (replace with your preprocessing logic)\ndef preprocess_image(img_path):\n    # Load and preprocess your image (example assumes resizing to 224x224)\n    img = Image.open(img_path)\n    img = img.convert('RGB')  # Ensure RGB mode\n    img = img.resize((224, 224))  # Resize to model input size\n    img = np.array(img)  # Convert PIL image to numpy array\n    return img\n\n# Example prediction function (replace with your model prediction function)\ndef predict_fn(images):\n    # Assuming 'model' is your trained Keras model\n    return model.predict(images)\n\n# Select an image to explain\nimg_path = '/kaggle/working/Image Files/Shenzhen_Custom_Masks/Normal/CHNCXR_0078_0.png'\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0010_0.png\"\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0028_0.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0327_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0333_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0360_1.png\"\nimg = preprocess_image(img_path)\n\n# Ensure the image is in the correct shape for LIME\nimg = img.astype(np.float32) / 255.0  # Normalize to [0, 1] if needed\n\n# Explain the prediction\nexplanation = explainer.explain_instance(\n    image=img,\n    classifier_fn=predict_fn,\n    top_labels=1,\n    hide_color=0,\n    num_samples=1000\n)\n\n# Get the explanation for the top class\ntemp, mask = explanation.get_image_and_mask(\n    label=explanation.top_labels[0],\n    positive_only=False,\n    num_features=5,\n    hide_rest=False\n)\n\n# Overlay the explanation mask on the original image\nimg_with_mask = mark_boundaries(np.array(img), mask)\n\n# Show the original image with overlaid ROI\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(np.array(img))\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(img_with_mask)\nplt.title('Image with ROI')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\nimport shap\nimport matplotlib.pyplot as plt\n\n# Define the image size\nimg_size = (224, 224)  # Adjust this to match your model's input size\n\n# Define the image path\nimg_path = '/kaggle/working/Image Files/Shenzhen_Custom_Masks/Normal/CHNCXR_0078_0.png'\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0010_0.png\"\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0028_0.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0327_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0333_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0360_1.png\"\n\n# Load and preprocess the image\nimg = image.load_img(img_path, target_size=img_size)\nimg_array = image.img_to_array(img)\nimg_array = np.expand_dims(img_array, axis=0)\nimg_array = tf.keras.applications.efficientnet.preprocess_input(img_array)\n\n# Make a prediction\npredictions = model.predict(img_array)\n\n# Get the class with the highest probability\npredicted_class = np.argmax(predictions, axis=1)\n\n# Assuming you have a list of class names\nclass_names = list(train_gen.class_indices.keys())  # Ensure train_gen is defined\npredicted_class_name = class_names[predicted_class[0]]\n\nprint(f'The predicted class is: {predicted_class_name}')\n\n# Define the function for SHAP\ndef f(x):\n    tmp = x.copy()\n    return model(tmp)\n\n# Create the masker using the shape of the loaded image\nmasker_blur = shap.maskers.Image(\"blur(224,224)\", img_array.shape[1:])\n\n# Create the explainer\nexplainer = shap.Explainer(f, masker_blur, output_names=class_names)\n\n# Compute SHAP values\nshap_values = explainer(img_array, max_evals=5000, batch_size=50)\n\n# Normalize SHAP values to fall within the range [0, 1]\n#shap_values.values = np.clip(shap_values.values, -1, 1)\n#shap_values.values = (shap_values.values - shap_values.values.min()) / (shap_values.values.max() - shap_values.values.min())\n\n# Normalize the image array to [0, 1] range for plotting\nimg_array_display = (img_array - img_array.min()) / (img_array.max() - img_array.min())\n\n# Plot the SHAP values\nshap.image_plot(shap_values, img_array_display, labels=class_names)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef make_gradcampp_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    grad_model = tf.keras.models.Model(\n        [model.inputs],\n        [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    with tf.GradientTape() as tape1:\n        with tf.GradientTape() as tape2:\n            with tf.GradientTape() as tape3:\n                last_conv_layer_output, preds = grad_model(img_array)\n                if pred_index is None:\n                    pred_index = tf.argmax(preds[0])\n                class_channel = preds[:, pred_index]\n\n            grads = tape3.gradient(class_channel, last_conv_layer_output)\n        grads2 = tape2.gradient(class_channel, last_conv_layer_output)\n    grads3 = tape1.gradient(class_channel, last_conv_layer_output)\n\n    alpha_num = grads2**2\n    alpha_denom = 2 * grads2**2 + tf.reduce_sum(last_conv_layer_output * grads3, axis=(1, 2))\n    alpha_denom = tf.where(alpha_denom != 0, alpha_denom, tf.ones(alpha_denom.shape))\n    alphas = alpha_num / alpha_denom\n    weights = tf.reduce_sum(alphas * tf.maximum(grads, 0), axis=(1, 2))\n\n    last_conv_layer_output = last_conv_layer_output[0]\n    cam = tf.reduce_sum(weights[:, tf.newaxis, tf.newaxis] * last_conv_layer_output, axis=-1)\n\n    heatmap = tf.maximum(cam, 0) / tf.math.reduce_max(cam)\n    heatmap = tf.image.resize(heatmap[..., tf.newaxis], (img_array.shape[1], img_array.shape[2]))\n    heatmap = tf.squeeze(heatmap)\n\n    return heatmap.numpy()\n\ndef display_gradcam(img_path, heatmap, alpha=0.4, threshold_percentile=90):\n    # Load and preprocess the original image\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Resize heatmap to match the original image size\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n\n    # Apply dynamic threshold to the heatmap based on percentile\n    threshold_value = np.percentile(heatmap, threshold_percentile)\n    heatmap = np.maximum(heatmap - threshold_value, 0)\n    heatmap = heatmap / np.max(heatmap)  # Normalize the heatmap\n\n    # Apply colormap to the heatmap\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)  # Convert heatmap to RGB for blending\n\n    # Blend heatmap with the original image\n    blended_img = (heatmap * alpha) + (img * (1 - alpha))\n    blended_img = np.clip(blended_img, 0, 255).astype(np.uint8)\n\n    # Plotting\n    plt.figure(figsize=(15, 5))\n\n    # Plot original image\n    plt.subplot(1, 3, 1)\n    plt.imshow(img)\n    plt.title('Original Image')\n    plt.axis('off')\n\n    # Plot heatmap\n    plt.subplot(1, 3, 2)\n    plt.imshow(heatmap)\n    plt.title('Heatmap')\n    plt.axis('off')\n\n    # Plot original image with heatmap overlay\n    plt.subplot(1, 3, 3)\n    plt.imshow(blended_img)\n    plt.title('Image with Heatmap Overlay')\n    plt.axis('off')\n\n    plt.show()\n\ndef get_last_conv_layer_name(model):\n    for layer in reversed(model.layers):\n        if isinstance(layer, tf.keras.layers.Conv2D):\n            return layer.name\n    return None\n\n# Example usage\nimg_path = '/kaggle/working/Image Files/Shenzhen_Custom_Masks/Normal/CHNCXR_0078_0.png'\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0010_0.png\"\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0028_0.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0327_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0333_1.png\"\n#img_path = \"/content/Shenzhen/Tuberculosis/CHNCXR_0360_1.png\"\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\n# Assuming 'base_model' is your model instance\nlast_conv_layer_name = get_last_conv_layer_name(base_model)\nprint(\"Last convolutional layer name:\", last_conv_layer_name)\n\n# Create the heatmap\nheatmap = make_gradcampp_heatmap(x, base_model, last_conv_layer_name)\n\ndisplay_gradcam(img_path, heatmap)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# # # 100","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom lime import lime_image\nfrom skimage.segmentation import mark_boundaries\nfrom PIL import Image\n\n# Initialize LIME image explainer\nexplainer = lime_image.LimeImageExplainer()\n\n# Example preprocessing function (replace with your preprocessing logic)\ndef preprocess_image(img_path):\n    # Load and preprocess your image (example assumes resizing to 224x224)\n    img = Image.open(img_path)\n    img = img.convert('RGB')  # Ensure RGB mode\n    img = img.resize((224, 224))  # Resize to model input size\n    img = np.array(img)  # Convert PIL image to numpy array\n    return img\n\n# Example prediction function (replace with your model prediction function)\ndef predict_fn(images):\n    # Assuming 'model' is your trained Keras model\n    return model.predict(images)\n\n# Select an image to explain\nimg_path = '/kaggle/working/Image Files/Shenzhen_Custom_Masks/Normal/CHNCXR_0100_0.png'\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0010_0.png\"\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0028_0.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0327_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0333_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0360_1.png\"\nimg = preprocess_image(img_path)\n\n# Ensure the image is in the correct shape for LIME\nimg = img.astype(np.float32) / 255.0  # Normalize to [0, 1] if needed\n\n# Explain the prediction\nexplanation = explainer.explain_instance(\n    image=img,\n    classifier_fn=predict_fn,\n    top_labels=1,\n    hide_color=0,\n    num_samples=1000\n)\n\n# Get the explanation for the top class\ntemp, mask = explanation.get_image_and_mask(\n    label=explanation.top_labels[0],\n    positive_only=False,\n    num_features=5,\n    hide_rest=False\n)\n\n# Overlay the explanation mask on the original image\nimg_with_mask = mark_boundaries(np.array(img), mask)\n\n# Show the original image with overlaid ROI\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(np.array(img))\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(img_with_mask)\nplt.title('Image with ROI')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\nimport shap\nimport matplotlib.pyplot as plt\n\n# Define the image size\nimg_size = (224, 224)  # Adjust this to match your model's input size\n\n# Define the image path\nimg_path = '/kaggle/working/Image Files/Shenzhen_Custom_Masks/Normal/CHNCXR_0100_0.png'\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0010_0.png\"\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0028_0.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0327_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0333_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0360_1.png\"\n\n# Load and preprocess the image\nimg = image.load_img(img_path, target_size=img_size)\nimg_array = image.img_to_array(img)\nimg_array = np.expand_dims(img_array, axis=0)\nimg_array = tf.keras.applications.efficientnet.preprocess_input(img_array)\n\n# Make a prediction\npredictions = model.predict(img_array)\n\n# Get the class with the highest probability\npredicted_class = np.argmax(predictions, axis=1)\n\n# Assuming you have a list of class names\nclass_names = list(train_gen.class_indices.keys())  # Ensure train_gen is defined\npredicted_class_name = class_names[predicted_class[0]]\n\nprint(f'The predicted class is: {predicted_class_name}')\n\n# Define the function for SHAP\ndef f(x):\n    tmp = x.copy()\n    return model(tmp)\n\n# Create the masker using the shape of the loaded image\nmasker_blur = shap.maskers.Image(\"blur(224,224)\", img_array.shape[1:])\n\n# Create the explainer\nexplainer = shap.Explainer(f, masker_blur, output_names=class_names)\n\n# Compute SHAP values\nshap_values = explainer(img_array, max_evals=5000, batch_size=50)\n\n# Normalize SHAP values to fall within the range [0, 1]\n#shap_values.values = np.clip(shap_values.values, -1, 1)\n#shap_values.values = (shap_values.values - shap_values.values.min()) / (shap_values.values.max() - shap_values.values.min())\n\n# Normalize the image array to [0, 1] range for plotting\nimg_array_display = (img_array - img_array.min()) / (img_array.max() - img_array.min())\n\n# Plot the SHAP values\nshap.image_plot(shap_values, img_array_display, labels=class_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef make_gradcampp_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    grad_model = tf.keras.models.Model(\n        [model.inputs],\n        [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    with tf.GradientTape() as tape1:\n        with tf.GradientTape() as tape2:\n            with tf.GradientTape() as tape3:\n                last_conv_layer_output, preds = grad_model(img_array)\n                if pred_index is None:\n                    pred_index = tf.argmax(preds[0])\n                class_channel = preds[:, pred_index]\n\n            grads = tape3.gradient(class_channel, last_conv_layer_output)\n        grads2 = tape2.gradient(class_channel, last_conv_layer_output)\n    grads3 = tape1.gradient(class_channel, last_conv_layer_output)\n\n    alpha_num = grads2**2\n    alpha_denom = 2 * grads2**2 + tf.reduce_sum(last_conv_layer_output * grads3, axis=(1, 2))\n    alpha_denom = tf.where(alpha_denom != 0, alpha_denom, tf.ones(alpha_denom.shape))\n    alphas = alpha_num / alpha_denom\n    weights = tf.reduce_sum(alphas * tf.maximum(grads, 0), axis=(1, 2))\n\n    last_conv_layer_output = last_conv_layer_output[0]\n    cam = tf.reduce_sum(weights[:, tf.newaxis, tf.newaxis] * last_conv_layer_output, axis=-1)\n\n    heatmap = tf.maximum(cam, 0) / tf.math.reduce_max(cam)\n    heatmap = tf.image.resize(heatmap[..., tf.newaxis], (img_array.shape[1], img_array.shape[2]))\n    heatmap = tf.squeeze(heatmap)\n\n    return heatmap.numpy()\n\ndef display_gradcam(img_path, heatmap, alpha=0.4, threshold_percentile=90):\n    # Load and preprocess the original image\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Resize heatmap to match the original image size\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n\n    # Apply dynamic threshold to the heatmap based on percentile\n    threshold_value = np.percentile(heatmap, threshold_percentile)\n    heatmap = np.maximum(heatmap - threshold_value, 0)\n    heatmap = heatmap / np.max(heatmap)  # Normalize the heatmap\n\n    # Apply colormap to the heatmap\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)  # Convert heatmap to RGB for blending\n\n    # Blend heatmap with the original image\n    blended_img = (heatmap * alpha) + (img * (1 - alpha))\n    blended_img = np.clip(blended_img, 0, 255).astype(np.uint8)\n\n    # Plotting\n    plt.figure(figsize=(15, 5))\n\n    # Plot original image\n    plt.subplot(1, 3, 1)\n    plt.imshow(img)\n    plt.title('Original Image')\n    plt.axis('off')\n\n    # Plot heatmap\n    plt.subplot(1, 3, 2)\n    plt.imshow(heatmap)\n    plt.title('Heatmap')\n    plt.axis('off')\n\n    # Plot original image with heatmap overlay\n    plt.subplot(1, 3, 3)\n    plt.imshow(blended_img)\n    plt.title('Image with Heatmap Overlay')\n    plt.axis('off')\n\n    plt.show()\n\ndef get_last_conv_layer_name(model):\n    for layer in reversed(model.layers):\n        if isinstance(layer, tf.keras.layers.Conv2D):\n            return layer.name\n    return None\n\n# Example usage\nimg_path = '/kaggle/working/Image Files/Shenzhen_Custom_Masks/Normal/CHNCXR_0100_0.png'\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0010_0.png\"\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0028_0.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0327_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0333_1.png\"\n#img_path = \"/content/Shenzhen/Tuberculosis/CHNCXR_0360_1.png\"\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\n# Assuming 'base_model' is your model instance\nlast_conv_layer_name = get_last_conv_layer_name(base_model)\nprint(\"Last convolutional layer name:\", last_conv_layer_name)\n\n# Create the heatmap\nheatmap = make_gradcampp_heatmap(x, base_model, last_conv_layer_name)\n\ndisplay_gradcam(img_path, heatmap)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# # # # 350","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom lime import lime_image\nfrom skimage.segmentation import mark_boundaries\nfrom PIL import Image\n\n# Initialize LIME image explainer\nexplainer = lime_image.LimeImageExplainer()\n\n# Example preprocessing function (replace with your preprocessing logic)\ndef preprocess_image(img_path):\n    # Load and preprocess your image (example assumes resizing to 224x224)\n    img = Image.open(img_path)\n    img = img.convert('RGB')  # Ensure RGB mode\n    img = img.resize((224, 224))  # Resize to model input size\n    img = np.array(img)  # Convert PIL image to numpy array\n    return img\n\n# Example prediction function (replace with your model prediction function)\ndef predict_fn(images):\n    # Assuming 'model' is your trained Keras model\n    return model.predict(images)\n\n# Select an image to explain\nimg_path = '/kaggle/working/Image Files/Shenzhen_Custom_Masks/Tuberculosis/CHNCXR_0350_1.png'\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0010_0.png\"\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0028_0.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0327_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0333_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0360_1.png\"\nimg = preprocess_image(img_path)\n\n# Ensure the image is in the correct shape for LIME\nimg = img.astype(np.float32) / 255.0  # Normalize to [0, 1] if needed\n\n# Explain the prediction\nexplanation = explainer.explain_instance(\n    image=img,\n    classifier_fn=predict_fn,\n    top_labels=1,\n    hide_color=0,\n    num_samples=1000\n)\n\n# Get the explanation for the top class\ntemp, mask = explanation.get_image_and_mask(\n    label=explanation.top_labels[0],\n    positive_only=False,\n    num_features=5,\n    hide_rest=False\n)\n\n# Overlay the explanation mask on the original image\nimg_with_mask = mark_boundaries(np.array(img), mask)\n\n# Show the original image with overlaid ROI\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.imshow(np.array(img))\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(img_with_mask)\nplt.title('Image with ROI')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\nimport shap\nimport matplotlib.pyplot as plt\n\n# Define the image size\nimg_size = (224, 224)  # Adjust this to match your model's input size\n\n# Define the image path\nimg_path = '/kaggle/working/Image Files/Shenzhen_Custom_Masks/Tuberculosis/CHNCXR_0350_1.png'\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0010_0.png\"\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0028_0.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0327_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0333_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0360_1.png\"\n\n# Load and preprocess the image\nimg = image.load_img(img_path, target_size=img_size)\nimg_array = image.img_to_array(img)\nimg_array = np.expand_dims(img_array, axis=0)\nimg_array = tf.keras.applications.efficientnet.preprocess_input(img_array)\n\n# Make a prediction\npredictions = model.predict(img_array)\n\n# Get the class with the highest probability\npredicted_class = np.argmax(predictions, axis=1)\n\n# Assuming you have a list of class names\nclass_names = list(train_gen.class_indices.keys())  # Ensure train_gen is defined\npredicted_class_name = class_names[predicted_class[0]]\n\nprint(f'The predicted class is: {predicted_class_name}')\n\n# Define the function for SHAP\ndef f(x):\n    tmp = x.copy()\n    return model(tmp)\n\n# Create the masker using the shape of the loaded image\nmasker_blur = shap.maskers.Image(\"blur(224,224)\", img_array.shape[1:])\n\n# Create the explainer\nexplainer = shap.Explainer(f, masker_blur, output_names=class_names)\n\n# Compute SHAP values\nshap_values = explainer(img_array, max_evals=5000, batch_size=50)\n\n# Normalize SHAP values to fall within the range [0, 1]\n#shap_values.values = np.clip(shap_values.values, -1, 1)\n#shap_values.values = (shap_values.values - shap_values.values.min()) / (shap_values.values.max() - shap_values.values.min())\n\n# Normalize the image array to [0, 1] range for plotting\nimg_array_display = (img_array - img_array.min()) / (img_array.max() - img_array.min())\n\n# Plot the SHAP values\nshap.image_plot(shap_values, img_array_display, labels=class_names)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\ndef make_gradcampp_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    grad_model = tf.keras.models.Model(\n        [model.inputs],\n        [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    with tf.GradientTape() as tape1:\n        with tf.GradientTape() as tape2:\n            with tf.GradientTape() as tape3:\n                last_conv_layer_output, preds = grad_model(img_array)\n                if pred_index is None:\n                    pred_index = tf.argmax(preds[0])\n                class_channel = preds[:, pred_index]\n\n            grads = tape3.gradient(class_channel, last_conv_layer_output)\n        grads2 = tape2.gradient(class_channel, last_conv_layer_output)\n    grads3 = tape1.gradient(class_channel, last_conv_layer_output)\n\n    alpha_num = grads2**2\n    alpha_denom = 2 * grads2**2 + tf.reduce_sum(last_conv_layer_output * grads3, axis=(1, 2))\n    alpha_denom = tf.where(alpha_denom != 0, alpha_denom, tf.ones(alpha_denom.shape))\n    alphas = alpha_num / alpha_denom\n    weights = tf.reduce_sum(alphas * tf.maximum(grads, 0), axis=(1, 2))\n\n    last_conv_layer_output = last_conv_layer_output[0]\n    cam = tf.reduce_sum(weights[:, tf.newaxis, tf.newaxis] * last_conv_layer_output, axis=-1)\n\n    heatmap = tf.maximum(cam, 0) / tf.math.reduce_max(cam)\n    heatmap = tf.image.resize(heatmap[..., tf.newaxis], (img_array.shape[1], img_array.shape[2]))\n    heatmap = tf.squeeze(heatmap)\n\n    return heatmap.numpy()\n\ndef display_gradcam(img_path, heatmap, alpha=0.4, threshold_percentile=90):\n    # Load and preprocess the original image\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Resize heatmap to match the original image size\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n\n    # Apply dynamic threshold to the heatmap based on percentile\n    threshold_value = np.percentile(heatmap, threshold_percentile)\n    heatmap = np.maximum(heatmap - threshold_value, 0)\n    heatmap = heatmap / np.max(heatmap)  # Normalize the heatmap\n\n    # Apply colormap to the heatmap\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)  # Convert heatmap to RGB for blending\n\n    # Blend heatmap with the original image\n    blended_img = (heatmap * alpha) + (img * (1 - alpha))\n    blended_img = np.clip(blended_img, 0, 255).astype(np.uint8)\n\n    # Plotting\n    plt.figure(figsize=(15, 5))\n\n    # Plot original image\n    plt.subplot(1, 3, 1)\n    plt.imshow(img)\n    plt.title('Original Image')\n    plt.axis('off')\n\n    # Plot heatmap\n    plt.subplot(1, 3, 2)\n    plt.imshow(heatmap)\n    plt.title('Heatmap')\n    plt.axis('off')\n\n    # Plot original image with heatmap overlay\n    plt.subplot(1, 3, 3)\n    plt.imshow(blended_img)\n    plt.title('Image with Heatmap Overlay')\n    plt.axis('off')\n\n    plt.show()\n\ndef get_last_conv_layer_name(model):\n    for layer in reversed(model.layers):\n        if isinstance(layer, tf.keras.layers.Conv2D):\n            return layer.name\n    return None\n\n# Example usage\nimg_path = '/kaggle/working/Image Files/Shenzhen_Custom_Masks/Tuberculosis/CHNCXR_0350_1.png'\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0010_0.png\"\n#img_path=\"/content/Shenzhen/Normal/CHNCXR_0028_0.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0327_1.png\"\n#img_path=\"/content/Shenzhen/Tuberculosis/CHNCXR_0333_1.png\"\n#img_path = \"/content/Shenzhen/Tuberculosis/CHNCXR_0360_1.png\"\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\n# Assuming 'base_model' is your model instance\nlast_conv_layer_name = get_last_conv_layer_name(base_model)\nprint(\"Last convolutional layer name:\", last_conv_layer_name)\n\n# Create the heatmap\nheatmap = make_gradcampp_heatmap(x, base_model, last_conv_layer_name)\n\ndisplay_gradcam(img_path, heatmap)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# # Effect of pre processing","metadata":{}},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\n\n# Define the image path\nimage_path = '/kaggle/working/Image Files/S+M_Custom/Tuberculosis/CHNCXR_0329_1.png'\n\n# Load the original image in grayscale\noriginal_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n# Check if the image was successfully loaded\nif original_image is None:\n    print(f\"Image at path '{image_path}' not found.\")\nelse:\n    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    clahe_image = clahe.apply(original_image)\n\n    # Apply histogram equalization on the original image (not on CLAHE result)\n    hist_eq_image = cv2.equalizeHist(original_image)\n\n    # Apply CLAHE and then histogram equalization\n    hist_eq_then_clahe_image = clahe.apply(hist_eq_image)\n\n    # Save the modified image, replacing the original\n    cv2.imwrite(image_path, hist_eq_then_clahe_image)\n\n    # Create a figure and axes to display the images side by side\n    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n    # Display original image on the first axis\n    axes[0].imshow(original_image, cmap='gray')\n    axes[0].set_title('Original Image')\n    axes[0].axis('off')\n\n    # Display processed image on the second axis\n    axes[1].imshow(hist_eq_then_clahe_image, cmap='gray')\n    axes[1].set_title('Processed Image')\n    axes[1].axis('off')\n\n    # Adjust layout and show the figure\n    plt.tight_layout()\n    plt.show()\n\n    print(\"Image processing complete.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}